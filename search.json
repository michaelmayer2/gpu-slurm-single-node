[
  {
    "objectID": "GPU-SLURM.html",
    "href": "GPU-SLURM.html",
    "title": "GPU and SLURM with Posit Workbench",
    "section": "",
    "text": "With the ever increasing popularity of AI, more and more customers are exploring the use of GPU’s with Posit Workbench. Many larger customers today have a Kubernetes or SLURM based Workbench environment for scalability. Those environments can be easily configured to allocate GPU’s to Workbench sessions and general compute jobs as consumable resource.\nSmaller to medium sized customers however sturggle a bit with the use of GPUs. In many cases they only can afford a single GPU on a server, many of them are still working on-prem but still want to make GPU computing available to their data scientists.\nThe below describes a possibility on how to configure a server with a single GPU so that it can be leveraged as a GPU server for a group of data scientists that can get shared access to the same GPU resources.\nWhile the general description and all testing for this document was done on AWS, the same approach can easily be converted into an on-prem installation procedure."
  },
  {
    "objectID": "GPU-SLURM.html#introduction",
    "href": "GPU-SLURM.html#introduction",
    "title": "GPU and SLURM with Posit Workbench",
    "section": "",
    "text": "With the ever increasing popularity of AI, more and more customers are exploring the use of GPU’s with Posit Workbench. Many larger customers today have a Kubernetes or SLURM based Workbench environment for scalability. Those environments can be easily configured to allocate GPU’s to Workbench sessions and general compute jobs as consumable resource.\nSmaller to medium sized customers however sturggle a bit with the use of GPUs. In many cases they only can afford a single GPU on a server, many of them are still working on-prem but still want to make GPU computing available to their data scientists.\nThe below describes a possibility on how to configure a server with a single GPU so that it can be leveraged as a GPU server for a group of data scientists that can get shared access to the same GPU resources.\nWhile the general description and all testing for this document was done on AWS, the same approach can easily be converted into an on-prem installation procedure."
  },
  {
    "objectID": "GPU-SLURM.html#general-information-for-our-setup",
    "href": "GPU-SLURM.html#general-information-for-our-setup",
    "title": "GPU and SLURM with Posit Workbench",
    "section": "General Information for our setup",
    "text": "General Information for our setup\nFor the detailed description below we will be using an AWS GPU instance and a standard AMI for Rocky Linux 9.3.\nWe will be using the AWS CLI tools to create a p3.2xlarge instance with some user-data script. In this userdata script, R, Python and Workbench will be installed using Posit provided binaries. SLURM will be compiled from source in the version specified.\n\nAWS setup\nFirst of all we need to define various parameters of our AWS account\n\n\n\n\n\n\n\n\nParameter name\nDescription\nExample\n\n\n\n\nVPC_ID\nID of VPC to be used\nvpc-1486376d\n\n\nSUBNET_ID\nID of a public subnet\nsubnet-cd7e8c86\n\n\nAMI_ID\nID of an AWS AMI that contains Rocky Linux 9\nami-05a40a9d755b0f73a\n\n\nKEY_PAIR\nName of SSH key pair to be used for logging into the EC2 instance\nmichael.mayer@posit.co-keypair-for-pulumi\n\n\nPOSIT_TAGS\nTags to be used for AWS resource cretion\n\"{Key=rs:project,Value=solutions}, \\             {Key=rs:environment,Value=development}, \\             {Key=rs:owner,Value=michael.mayer@posit.co}\"\n\n\n\nOnce those parameters are defined we can create a security group to allow ingress on port 22 (ssh) and 8787 (workbench)\n\n\nCode\nSG_ID=`aws ec2 create-security-group \\\n    --group-name ssh-wb-sg \\\n    --description \"SG for Workbench (port 8787) and SSH (port 22) access\" \\\n    --tag-specifications \"ResourceType=security-group,\\\n        Tags=[{Key=Name,Value=ssh-wb-sg},${POSIT_TAGS}]\" \\\n    --vpc-id \"${VPC_ID}\" | jq -r '.GroupId' `\n\naws ec2 authorize-security-group-ingress \\\n    --group-id \"${SG_ID}\" \\\n    --protocol tcp \\\n    --port 8787 \\\n    --cidr \"0.0.0.0/0\"\n\naws ec2 authorize-security-group-ingress \\\n    --group-id \"${SG_ID}\" \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr \"0.0.0.0/0\"\n\n\nand finally we can create the EC2 instance via\n\n\nCode\naws ec2 run-instances \\\n    --image-id $AMI_ID \\\n    --count 1 \\\n    --instance-type p3.2xlarge \\\n    --key-name $KEY_PAIR \\\n    --security-group-ids $SG_ID \\\n    --block-device-mappings \"[{\\\"DeviceName\\\":\\\"/dev/sda1\\\",\\\"Ebs\\\":{\\\"VolumeSize\\\":100,\\\"DeleteOnTermination\\\":true}}]\" \\\n    --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=rl9-gpu},${POSIT_TAGS}]\" 'ResourceType=volume,Tags=[{Key=Name,Value=rl9-gpu-disk}]' \\\n    --user-data file://${PWD}/user-data.sh\n\n\nIn your EC2 console you then will be able to see a new server with Name rl9-gpu appearing - once it is up and running, you can connect to it to check progress. Please note the server will reboot once before fully ready. This is needed to rebuild the nvidia kernel modules for the linux kernel.\n\n\nuser-data script\n\nIntroduction and Overview\n\n\n\n\n\n\n\n\nParameter name\nDescription\nDefault value\n\n\n\n\nR_VERSION_LIST\nList of R versions to be installed\n\"3.6.3 4.0.5 4.1.3 4.2.3 4.3.3\"\n\n\nPYTHON_VERSION_LIST\nList of Python versions to be installed\n\"3.8.19 3.9.19 3.10.14 3.11.8\"\n\n\nSLURM_VERSION\nVersion of SLURM to be used\n23.11.5-1\n\n\nPWB_VERSION\nVersion of Posit Workbench to be used\n2024.04.0-daily-675.pro4\n\n\n\nAll of the above parameters can be modified at the start to the user-data script.\n\n\nSELinux\nFirst, we will turn SELinux into permissive mode - this is needed for Workbench to run properly (mainly issues in Launcher)\n\n\nCode\nsetenforce permissive\nsed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/sysconfig/selinux \nsed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config\ngrubby --update-kernel ALL --args selinux=permissive\n\n\n\n\nConfiguring the GPU\nThis is described in great detail at NVIDIA docs. It will install the NVIDIA driver (550 series) and CUDA 12.4.\n\n\nCode\ndnf install -y 'dnf-command(config-manager)' epel-release\ncrb enable\ndnf config-manager --add-repo \\\n  https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo\ndnf clean expire-cache\ndnf install -y dkms\ndnf module install -y nvidia-driver:550\ndnf install -y cuda-toolkit-12-4\ndnf install -y libcudnn8\n\ncat &lt;&lt; EOF &gt; /etc/profile.d/cuda.sh\nexport PATH=/usr/local/cuda/bin:\\$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:\\$LD_LIBRARY_PATH\nEOF\n\n\nThe above command will download about 5 GB of data. It will additionally install libcudnn8 and set both PATH and LD_LIBRARY_PATH so that the CUDA tools are usable. The libcudnn8 and LD_LIBRARY_PATH are important to make TensorFlow 2.16.1 work.\n\n\nSetting up R\nWe will install a list of R versions and configure each for Java support.\n\n\nCode\nexport R_VERSION_LIST=\"3.6.3 4.0.5 4.1.3 4.2.3 4.3.3\"\n\n# While CUDA seems to prefer JAVA 11, Binary R packages are typically compiled with Java 8 only. \ndnf -y install java-1.8.0-openjdk-devel\n\ncurl -O  https://raw.githubusercontent.com/sol-eng/singularity-rstudio/main/data/r-session-complete/centos7/scripts/run.R\ncurl -O  https://raw.githubusercontent.com/sol-eng/singularity-rstudio/main/data/r-session-complete/centos7/scripts/bioc.txt\n\nfor R_VERSION in $R_VERSION_LIST; do yum install -y https://cdn.rstudio.com/r/rhel-9/pkgs/R-${R_VERSION}-1-1.x86_64.rpm; done\n\nfor R_VERSION in $R_VERSION_LIST; do \\\n        export PATH=/opt/R/$R_VERSION/bin:$PATH && \\\n        export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/ && \\\n        /opt/R/${R_VERSION}/bin/R CMD javareconf \n\n\n\n\nSetting up Python\nWe will install a list of Python versions and configure JupyterLab for the last python version mentioned in PYTHON_VERSION_LIST.\n\n\nCode\n# Python installation \n\nif [ -z $PYTHON_VERSION_LIST ]; then PYTHON_VERSION_LIST=\"3.8.19 3.9.19 3.10.14 3.11.8\"; fi\n\nfor PYTHON_VERSION in $PYTHON_VERSION_LIST\ndo\n    yum install -y https://cdn.rstudio.com/python/rhel-9/pkgs/python-${PYTHON_VERSION}-1-1.x86_64.rpm\ndone\n\n# Python version to be used for jupyter install (last one of PYTHON_VERSION_LIST)\nTMPVAR=($PYTHON_VERSION_LIST)\nPYTHON_VERSION=${TMPVAR[@]: -1}\n# JupyterLab Version selected \nJUPYTERLAB_VERSION=3.6.5\n\n/opt/python/\"${PYTHON_VERSION}\"/bin/python -m venv /opt/python/jupyter \\\n    && /opt/python/jupyter/bin/pip install \\\n      jupyter \\\n      jupyterlab==\"${JUPYTERLAB_VERSION}\" \\\n      rsconnect_jupyter \\\n      rsconnect_python \\\n      rsp_jupyter \\\n      workbench_jupyterlab \\\n    && ln -s /opt/python/jupyter/bin/jupyter /usr/local/bin/jupyter \\\n    && /opt/python/jupyter/bin/jupyter-nbextension install --sys-prefix --py rsp_jupyter \\\n    && /opt/python/jupyter/bin/jupyter-nbextension enable --sys-prefix --py rsp_jupyter \\\n    && /opt/python/jupyter/bin/jupyter-nbextension install --sys-prefix --py rsconnect_jupyter \\\n    && /opt/python/jupyter/bin/jupyter-nbextension enable --sys-prefix --py rsconnect_jupyter \\\n    && /opt/python/jupyter/bin/jupyter-serverextension enable --sys-prefix --py rsconnect_jupyter\n\n\n\n\nBuilding SLURM from sources\nWe will build SLURM from source to ensure we can use the latest version and do not depend on any OS vendor to package SLURM. The minimum version of SLURM we can use is SLURM 22.x due to our use of Sharding.\nThe SLURM config uses\n\nPrefix /usr/local\nCLI binaries in /usr/local/bin (Note the script to expand PATH below)\nslurm.conf in /usr/local/etc/slurm\n\n\n\nexternal accounting DB (MariaDB)\nMUNGE authentication\ncgroups for cpus, memory and devices (e.g. GPUs)\nall log files are collected in /var/log/slurm/\ngres.conf for our GPU and 8 shards.\n\n\n\nCode\nif [ -z $SLURM_VERSION ]; then export SLURM_VERSION=23.11.5-1\n\ndnf install -y git\n\ngroupadd -r --gid=105 munge\nuseradd -r -s /bin/bash -g munge --uid=105 munge\n\ndnf install -y bash-completion mariadb-devel mariadb-server hdf5-devel munge munge-devel dbus-devel hwloc-devel readline-devel lua-devel man2html\n\nif [ -f /usr/sbin/create-munge-key ]; then /usr/sbin/create-munge-key ; else /usr/sbin/mungekey -c -f; fi\n\nexport SLURM_PREFIX=/usr/local\nmkdir -p $SLURM_PREFIX/etc/slurm && mkdir -p /tmp/build && cd /tmp/build \\\n    && bash -c \"git clone --depth 1 -b slurm-\\${SLURM_VERSION//./-} https://github.com/SchedMD/slurm.git\" \\\n    && cd slurm \\\n    && echo \"Configuring SLURM ${SLURM_VERSION}\" \\\n    && bash -c \"./configure --enable-debug --prefix=$SLURM_PREFIX --sysconfdir=$SLURM_PREFIX/etc/slurm \\\n        --with-mysql_config=/usr/bin  --libdir=$SLURM_PREFIX/lib64 --with-systemdsystemunitdir=/usr/lib/systemd/system/ &gt;& $SLURM_PREFIX/etc/slurm/.configure.log\"  \\\n    && echo \"Building SLURM ${SLURM_VERSION}\" \\\n    && bash -c \"make -j 8 &gt;& $SLURM_PREFIX/etc/slurm/.build.log\" \\\n    && echo \"Installing SLURM ${SLURM_VERSION}\" \\\n    && bash -c \"make -j 8 install &gt;& $SLURM_PREFIX/etc/slurm/.install.log\" \\\n    && install -D -m644 contribs/slurm_completion_help/slurm_completion.sh /etc/profile.d/slurm_completion.sh \\\n    && cd .. \\\n    && rm -rf slurm \\\n    && groupadd -r --gid=980 slurm \\\n    && useradd -r -g slurm --uid=985 slurm \\\n    && mkdir -p /etc/sysconfig/slurm \\\n        /var/spool/slurmd \\\n        /var/run/slurm \\\n        /var/run/slurmdbd \\\n        /var/lib/slurmd \\\n        /var/log/slurm \\\n        /var/spool/slurmctld \\\n    && chown -R slurm:slurm /var/*/slurm*\n\n# make sure /usr/local/bin is in $PATH\necho \"export PATH=/usr/local/bin:\\$PATH\" &gt; /etc/profile.d/path-usr-local.sh\n\n# Configure mariadb\nsystemctl enable mariadb\nsystemctl start mariadb\n\nmysql_secure_installation &lt;&lt; EOF\n\nY\nY\nTestme1234\nTestme1234\nY\nY\nY\nY\nEOF\n\n# SLURM configuration\n\nmkdir -p /usr/local/etc/slurm/\ncat &lt;&lt; EOF &gt; /usr/local/etc/slurm/slurm.conf\nClusterName=pwb\nSlurmctldHost=localhost\nProctrackType=proctrack/cgroup\nReturnToService=1\nSlurmctldPidFile=/var/run/slurm/slurmctld.pid\nSlurmctldPort=6817\nSlurmdPidFile=/var/run/slurm/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/spool/slurmd\nSlurmUser=slurm\nStateSaveLocation=/var/spool/slurmctld\nTaskPlugin=task/affinity,task/cgroup\nInactiveLimit=0\nKillWait=30\nMinJobAge=300\nSlurmctldTimeout=120\nSlurmdTimeout=300\nWaittime=0\nSchedulerType=sched/backfill\nSelectType=select/cons_tres\nAccountingStorageType=accounting_storage/slurmdbd\nAccountingStorageTRES=gres/gpu,gres/shard\nGresTypes=gpu,shard\nJobCompType=jobcomp/none\nJobAcctGatherFrequency=10\nSlurmctldDebug=info\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nSlurmdDebug=info\nSlurmdLogFile=/var/log/slurm/slurmd.log\n#NodeName=localhost CPUs=8 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:1,shard:8 State=UNKNOWN\nNodeName=localhost CPUs=8 Boards=1 Gres=gpu:v100:1,shard:8 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=61022 State=UNKNOWN\nPartitionName=all Nodes=ALL Default=YES MaxTime=INFINITE State=UP\nEOF\n\ncat &lt;&lt; EOF &gt; /usr/local/etc/slurm/slurmdbd.conf\nAuthType=auth/munge\nLogFile=/var/log/slurm/slurmdbd.log\nPidFile=/var/run/slurm/slurmdbd.pid\nDbdPort=6819\nSlurmUser=slurm\nStorageType=accounting_storage/mysql\n\nDbdHost=localhost\nStorageHost=localhost\nStoragePort=3306\nStorageLoc=slurm\nStorageUser=root\nStoragePass=Testme1234\nEOF\n\ncat &lt;&lt; EOF &gt; /usr/local/etc/slurm/gres.conf\nAutoDetect=off\nNodeName=localhost Name=gpu Type=v100 File=/dev/nvidia0\nNodeName=localhost Name=shard Count=8\n#p3.8xlarge\n#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia0\n#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia1\n#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia3\n#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia2\n#NodeName=localhost Name=shard Count=8\nEOF\n\ncat &lt;&lt; EOF &gt; /usr/local/etc/slurm/cgroup.conf\nConstrainCores=yes\nConstrainDevices=yes\nConstrainRAMSpace=yes\nEOF\n\nchown slurm /usr/local/etc/slurm/slurmdbd.conf\nchmod 0600 /usr/local/etc/slurm/slurmdbd.conf\n\nsystemctl enable munge\nsystemctl enable slurmctld \nsystemctl enable slurmdbd\nsystemctl enable slurmd\n\n\n\n\nInstall and configure Workbench\nWorkbench will be configured for SLURM Launcher with GRES support only. This will allow the user to specify shard:X as GRES where X can run from 1 to 8. CPU and memory resources have been set up so that they are making most efficient use of the available CPUs and memory.\nWe also will configure support for all 4 IDE’s, e.g. RStudio IDE, VS Code, Jupyter Notebooks and JupyterLab. A user named rstudio is created that has all admin privileges on Workbench. A random 10-digit password will be assigned and saved in a .rstudio-pw file in the rstudio users’ home-directory.\n\n\nCode\n# Workbench installation \n\nif [ -z $PWB_VERSION ]; then export PWB_VERSION=2024.04.0-daily-675.pro4; fi\nyum -y install https://s3.amazonaws.com/rstudio-ide-build/server/rhel9/x86_64/rstudio-workbench-rhel-${PWB_VERSION}-x86_64.rpm\n\n# Workbench setup\n\nconfigdir=\"/etc/rstudio\"\n\n# Add SLURM integration \nmyip=`curl http://checkip.amazonaws.com`\n\nmkdir -p /opt/rstudio/shared-storage\n\ncat &gt; $configdir/launcher-env &lt;&lt; EOF\nRSTUDIO_DISABLE_PACKAGE_INSTALL_PROMPT=yes\nSLURM_CONF=/usr/local/etc/slurm.conf\nEOF\n \ncat &gt; $configdir/rserver.conf &lt;&lt; EOF\n\n# Launcher Config\nlauncher-address=127.0.0.1\nlauncher-port=5559\nlauncher-sessions-enabled=1\nlauncher-default-cluster=Slurm\nlauncher-sessions-callback-address=http://${myip}:8787\n\n# Disable R Versions scanning\n#r-versions-scan=0\n\nauth-pam-sessions-enabled=1\nauth-pam-sessions-use-password=1\n\n# Enable Admin Dashboard\nadmin-enabled=1\nadmin-group=rstudio-admins\nadmin-superuser-group=rstudio-superuser-admins\nadmin-monitor-log-use-server-time-zone=1\naudit-r-console-user-limit-mb=200\naudit-r-console-user-limit-months=3\n\n# Enable Auditing\naudit-r-console=all\naudit-r-sessions=1\naudit-r-sessions-limit-mb=512\naudit-r-sessions-limit-months=6\n\nEOF\n\ncat &gt; $configdir/launcher.conf&lt;&lt;EOF\n[server]\naddress=127.0.0.1\nport=5559\nserver-user=rstudio-server\nadmin-group=rstudio-server\nauthorization-enabled=1\nthread-pool-size=4\nenable-debug-logging=1\n\n[cluster]\nname=Slurm\ntype=Slurm\n\n#[cluster]\n#name=Local\n#type=Local\n\nEOF\n\n#cat &gt; $configdir/launcher.slurm.profiles.conf&lt;&lt;EOF \n#[*]\n#default-cpus=1\n#default-mem-mb=512\n##max-cpus=2\n#max-mem-mb=1024\n#EOF\n\ncat &gt; $configdir/launcher.slurm.resources.conf&lt;&lt;EOF\n[small]\nname = \"Small (1 cpu, 8 GB mem)\"\ncpus=1\nmem-mb=7627\n[medium]\nname = \"Medium (2 cpu, 16 GB mem)\"\ncpus=2\nmem-mb=15255\n[large]\nname = \"Large (4 cpu, 32 GB mem)\"\ncpus=4\nmem-mb=30511\n[xlarge]\nname = \"Large (8 cpu, 64 GB mem)\"\ncpus=4\nmem-mb=61022\nEOF\n\ncat &gt; $configdir/launcher.slurm.conf &lt;&lt; EOF \n# Enable debugging\nenable-debug-logging=1\n\n# Basic configuration\nslurm-service-user=slurm\nslurm-bin-path=/usr/local/bin\n\n# GPU specifics\nenable-gres=1\n#gpu-types=v100\n\nEOF\n\ncat &gt; $configdir/jupyter.conf &lt;&lt; EOF\njupyter-exe=/usr/local/bin/jupyter\nnotebooks-enabled=1\nlabs-enabled=1\nEOF\n\n# Install VSCode based on the PWB version.\nif ( rstudio-server | grep configure-vs-code ); then rstudio-server configure-vs-code ; rstudio-server install-vs-code-ext; else rstudio-server install-vs-code /opt/rstudio/vscode/; fi\n  \ncat &gt; $configdir/vscode.conf &lt;&lt; EOF\nenabled=1\nexe=/usr/lib/rstudio-server/bin/code-server/bin/code-server\nargs=--verbose --host=0.0.0.0 \nEOF\n\n# Add sample user \ngroupadd --gid 8787 rstudio\nuseradd -s /bin/bash -m --gid rstudio --uid 8787 rstudio\ngroupadd --gid 8788 rstudio-admins\ngroupadd --gid 8789 rstudio-superuser-admins\nusermod -G rstudio-admins,rstudio-superuser-admins rstudio\n\n# below SECRET string is a 10-digit random string that will be saved\nrspasswd = `tr -dc A-Za-z0-9 &lt; /dev/urandom | head -c 10; echo`\necho $rspasswd &gt; /home/rstudio/.rstudio-pw\necho -e \"$rspasswd\\n$rspasswd\" | passwd rstudio\n\n\n\n\nCouple of useful tools\n\n\nCode\n# Install a couple of useful tools\n\nyum install -y pciutils net-tools nc\n\n\n\n\nsystemctl dependencies\nA big part of the deployment is to ensure that the services are being started by systemd in the right order. For this, we tweak the systemd service definitions as follows:\n\nslurmdbd reports it has started but the service is still starting up. This needs to be caught by a ExecStartPost script that keeps polling until the slurmdbd port is actually open.\nslurmd will only start properly if the nvidia driver has been loaded and the device /dev/nvidia0 has been created.\nnvidia-persistenced ensures that the nvidia kernel module is loaded - it hence depends on dkms which on its own ensures the kernel module for nvidia is being recompiled for any new kernel.\nrstudio-server is only started after rstudio-launcher in order to avoid triggering any Connection refused messages from rstudio-server.\nrstudio-server will start only after slurmctld to ensure that SLURM Launcher is starting up properly.\n\n\n\nCode\n# Structure systemctl order of running services\n\nsed -i 's/After=.*/After=dkms.service/' /usr/lib/systemd/system/nvidia-persistenced.service \nsed -i 's/After=/&slurmdbd.service /' /usr/lib/systemd/system/slurmctld.service\nsed -i 's/After=/&slurmctld.service /' /usr/lib/systemd/system/slurmd.service\nsed -i '/After=.*/a ConditionPathExists=\\/dev\\/nvidia0' /usr/lib/systemd/system/slurmd.service\nsed -i '/ExecReload=.*/a ExecStartPost=\\/usr\\/bin\\/timeout 30 sh -c \"while ! ss -H -t -l -n sport = \\:6819 | grep -q ^LISTEN.*:6819; do sleep 1; done\"' /usr/lib/systemd/system/slurmdbd.service\nsed -i 's/After=/&rstudio-launcher.service /' /usr/lib/systemd/system/rstudio-server.service\nsed -i 's/After=/&slurmctld.service /' /usr/lib/systemd/system/rstudio-launcher.service\n\n\n\n\nReboot\nFinally we need to reboot the server in order to get the kernel module built and all the services started properly.\n\n\nCode\nreboot"
  }
]