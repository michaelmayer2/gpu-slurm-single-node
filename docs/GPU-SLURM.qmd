---
title: "GPU and SLURM with Posit Workbench"
author: "Michael Mayer"
format: 
  html:
    code-fold: true
    code-tools: true
execute:
  echo: true
  eval: false
editor: visual
---

## Introduction

With the ever increasing popularity of AI, more and more customers are exploring the use of GPU's with Posit Workbench. Many larger customers today have a Kubernetes or SLURM based Workbench environment for scalability. Those environments can be easily configured to allocate GPU's to Workbench sessions and general compute jobs as consumable resource.

Smaller to medium sized customers however sturggle a bit with the use of GPUs. In many cases they only can afford a single GPU on a server, many of them are still working on-prem but still want to make GPU computing available to their data scientists.

The below describes a possibility on how to configure a server with a single GPU so that it can be leveraged as a GPU server for a group of data scientists that can get shared access to the same GPU resources.

While the general description and all testing for this document was done on AWS, the same approach can easily be converted into an on-prem installation procedure.

## General Information for our setup

For the detailed description below we will be using an AWS GPU instance and a standard AMI for Rocky Linux 9.3.

We will be using the AWS CLI tools to create a `p3.2xlarge` instance with some user-data script. In this userdata script, [R](https://docs.posit.co/resources/install-r/), [Python](https://docs.posit.co/resources/install-python/) and [Workbench](https://docs.posit.co/ide/server-pro/getting_started/installation/installation.html#download-and-install) will be installed using Posit provided binaries. SLURM will be compiled from source in the version specified.

### AWS setup

First of all we need to define various parameters of our AWS account

| Parameter name | Description                                                       | Example                                                                                                                                               |
|------------------------|------------------------|------------------------|
| `VPC_ID`       | ID of VPC to be used                                              | `vpc-1486376d`                                                                                                                                        |
| `SUBNET_ID`    | ID of a public subnet                                             | `subnet-cd7e8c86`                                                                                                                                     |
| `AMI_ID`       | ID of an AWS AMI that contains Rocky Linux 9                      | `ami-05a40a9d755b0f73a`                                                                                                                               |
| `KEY_PAIR`     | Name of SSH key pair to be used for logging into the EC2 instance | `michael.mayer@posit.co-keypair-for-pulumi`                                                                                                           |
| `POSIT_TAGS`   | Tags to be used for AWS resource cretion                          | `"{Key=rs:project,Value=solutions}, \             {Key=rs:environment,Value=development}, \             {Key=rs:owner,Value=michael.mayer@posit.co}"` |

Once those parameters are defined we can create a security group to allow ingress on port 22 (ssh) and 8787 (workbench)

```{python}
SG_ID=`aws ec2 create-security-group \
    --group-name ssh-wb-sg \
    --description "SG for Workbench (port 8787) and SSH (port 22) access" \
    --tag-specifications "ResourceType=security-group,\
        Tags=[{Key=Name,Value=ssh-wb-sg},${POSIT_TAGS}]" \
    --vpc-id "${VPC_ID}" | jq -r '.GroupId' `

aws ec2 authorize-security-group-ingress \
    --group-id "${SG_ID}" \
    --protocol tcp \
    --port 8787 \
    --cidr "0.0.0.0/0"

aws ec2 authorize-security-group-ingress \
    --group-id "${SG_ID}" \
    --protocol tcp \
    --port 22 \
    --cidr "0.0.0.0/0"
```

and finally we can create the EC2 instance via

```{python}
aws ec2 run-instances \
    --image-id $AMI_ID \
    --count 1 \
    --instance-type p3.2xlarge \
    --key-name $KEY_PAIR \
    --security-group-ids $SG_ID \
    --block-device-mappings "[{\"DeviceName\":\"/dev/sda1\",\"Ebs\":{\"VolumeSize\":100,\"DeleteOnTermination\":true}}]" \
    --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=rl9-gpu},${POSIT_TAGS}]" 'ResourceType=volume,Tags=[{Key=Name,Value=rl9-gpu-disk}]' \
    --user-data file://${PWD}/user-data.sh
```

In your EC2 console you then will be able to see a new server with Name `rl9-gpu` appearing - once it is up and running, you can connect to it to check progress. Please note the server will reboot once before fully ready. This is needed to rebuild the nvidia kernel modules for the linux kernel.

### user-data script

#### Introduction and Overview

| Parameter name        | Description                             | Default value                     |
|------------------------|------------------------|------------------------|
| `R_VERSION_LIST`      | List of R versions to be installed      | `"3.6.3 4.0.5 4.1.3 4.2.3 4.3.3"` |
| `PYTHON_VERSION_LIST` | List of Python versions to be installed | `"3.8.19 3.9.19 3.10.14 3.11.8"`  |
| `SLURM_VERSION`       | Version of SLURM to be used             | `23.11.5-1`                       |
| `PWB_VERSION`         | Version of Posit Workbench to be used   | `2024.04.0-daily-675.pro4`        |

All of the above parameters can be modified at the start to the user-data script.

#### SELinux

First, we will turn SELinux into permissive mode - this is needed for Workbench to run properly (mainly issues in Launcher)

```{python}
setenforce permissive
sed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/sysconfig/selinux 
sed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config
grubby --update-kernel ALL --args selinux=permissive
```

#### Configuring the GPU

This is described in great detail at [NVIDIA docs](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#network-repo-installation-for-rhel-9-rocky-9). It will install the NVIDIA driver (550 series) and CUDA 12.4.

```{python}
dnf install -y 'dnf-command(config-manager)' epel-release
crb enable
dnf config-manager --add-repo \
  https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo
dnf clean expire-cache
dnf install -y dkms
dnf module install -y nvidia-driver:550
dnf install -y cuda-toolkit-12-4
dnf install -y libcudnn8

cat << EOF > /etc/profile.d/cuda.sh
export PATH=/usr/local/cuda/bin:\$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:\$LD_LIBRARY_PATH
EOF
```

The above command will download about 5 GB of data. It will additionally install `libcudnn8` and set both `PATH` and `LD_LIBRARY_PATH` so that the CUDA tools are usable. The `libcudnn8` and `LD_LIBRARY_PATH` are important to make [TensorFlow 2.16.1 work](https://github.com/tensorflow/tensorflow/issues/63362).

#### Setting up R

We will install a list of [R versions](https://docs.posit.co/resources/install-r/) and configure each for Java support.

```{python}
export R_VERSION_LIST="3.6.3 4.0.5 4.1.3 4.2.3 4.3.3"

# While CUDA seems to prefer JAVA 11, Binary R packages are typically compiled with Java 8 only. 
dnf -y install java-1.8.0-openjdk-devel

curl -O  https://raw.githubusercontent.com/sol-eng/singularity-rstudio/main/data/r-session-complete/centos7/scripts/run.R
curl -O  https://raw.githubusercontent.com/sol-eng/singularity-rstudio/main/data/r-session-complete/centos7/scripts/bioc.txt

for R_VERSION in $R_VERSION_LIST; do yum install -y https://cdn.rstudio.com/r/rhel-9/pkgs/R-${R_VERSION}-1-1.x86_64.rpm; done

for R_VERSION in $R_VERSION_LIST; do \
        export PATH=/opt/R/$R_VERSION/bin:$PATH && \
        export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/ && \
        /opt/R/${R_VERSION}/bin/R CMD javareconf 
```

#### Setting up Python

We will install a list of [Python versions](https://docs.posit.co/resources/install-python/) and configure JupyterLab for the last python version mentioned in `PYTHON_VERSION_LIST`.

```{python}
# Python installation 

if [ -z $PYTHON_VERSION_LIST ]; then PYTHON_VERSION_LIST="3.8.19 3.9.19 3.10.14 3.11.8"; fi

for PYTHON_VERSION in $PYTHON_VERSION_LIST
do
    yum install -y https://cdn.rstudio.com/python/rhel-9/pkgs/python-${PYTHON_VERSION}-1-1.x86_64.rpm
done

# Python version to be used for jupyter install (last one of PYTHON_VERSION_LIST)
TMPVAR=($PYTHON_VERSION_LIST)
PYTHON_VERSION=${TMPVAR[@]: -1}
# JupyterLab Version selected 
JUPYTERLAB_VERSION=3.6.5

/opt/python/"${PYTHON_VERSION}"/bin/python -m venv /opt/python/jupyter \
    && /opt/python/jupyter/bin/pip install \
      jupyter \
      jupyterlab=="${JUPYTERLAB_VERSION}" \
      rsconnect_jupyter \
      rsconnect_python \
      rsp_jupyter \
      workbench_jupyterlab \
    && ln -s /opt/python/jupyter/bin/jupyter /usr/local/bin/jupyter \
    && /opt/python/jupyter/bin/jupyter-nbextension install --sys-prefix --py rsp_jupyter \
    && /opt/python/jupyter/bin/jupyter-nbextension enable --sys-prefix --py rsp_jupyter \
    && /opt/python/jupyter/bin/jupyter-nbextension install --sys-prefix --py rsconnect_jupyter \
    && /opt/python/jupyter/bin/jupyter-nbextension enable --sys-prefix --py rsconnect_jupyter \
    && /opt/python/jupyter/bin/jupyter-serverextension enable --sys-prefix --py rsconnect_jupyter
```

#### Building SLURM from sources

We will build SLURM from source to ensure we can use the latest version and do not depend on any OS vendor to package SLURM. The minimum version of SLURM we can use is SLURM 22.x due to our use of [Sharding](https://slurm.schedmd.com/gres.html#Sharding).

The SLURM config uses

-   Prefix `/usr/local`

-   CLI binaries in `/usr/local/bin` (Note the script to expand `PATH` below)

-   `slurm.conf` in `/usr/local/etc/slurm`

<!-- -->

-   external accounting DB (MariaDB)

-   MUNGE authentication

-   `cgroups` for cpus, memory and devices (e.g. GPUs)

-   all log files are collected in `/var/log/slurm/`

-   `gres.conf` for our GPU and 8 shards.

```{python}
if [ -z $SLURM_VERSION ]; then export SLURM_VERSION=23.11.5-1

dnf install -y git

groupadd -r --gid=105 munge
useradd -r -s /bin/bash -g munge --uid=105 munge

dnf install -y bash-completion mariadb-devel mariadb-server hdf5-devel munge munge-devel dbus-devel hwloc-devel readline-devel lua-devel man2html

if [ -f /usr/sbin/create-munge-key ]; then /usr/sbin/create-munge-key ; else /usr/sbin/mungekey -c -f; fi

export SLURM_PREFIX=/usr/local
mkdir -p $SLURM_PREFIX/etc/slurm && mkdir -p /tmp/build && cd /tmp/build \
    && bash -c "git clone --depth 1 -b slurm-\${SLURM_VERSION//./-} https://github.com/SchedMD/slurm.git" \
    && cd slurm \
    && echo "Configuring SLURM ${SLURM_VERSION}" \
    && bash -c "./configure --enable-debug --prefix=$SLURM_PREFIX --sysconfdir=$SLURM_PREFIX/etc/slurm \
        --with-mysql_config=/usr/bin  --libdir=$SLURM_PREFIX/lib64 --with-systemdsystemunitdir=/usr/lib/systemd/system/ >& $SLURM_PREFIX/etc/slurm/.configure.log"  \
    && echo "Building SLURM ${SLURM_VERSION}" \
    && bash -c "make -j 8 >& $SLURM_PREFIX/etc/slurm/.build.log" \
    && echo "Installing SLURM ${SLURM_VERSION}" \
    && bash -c "make -j 8 install >& $SLURM_PREFIX/etc/slurm/.install.log" \
    && install -D -m644 contribs/slurm_completion_help/slurm_completion.sh /etc/profile.d/slurm_completion.sh \
    && cd .. \
    && rm -rf slurm \
    && groupadd -r --gid=980 slurm \
    && useradd -r -g slurm --uid=985 slurm \
    && mkdir -p /etc/sysconfig/slurm \
        /var/spool/slurmd \
        /var/run/slurm \
        /var/run/slurmdbd \
        /var/lib/slurmd \
        /var/log/slurm \
        /var/spool/slurmctld \
    && chown -R slurm:slurm /var/*/slurm*

# make sure /usr/local/bin is in $PATH
echo "export PATH=/usr/local/bin:\$PATH" > /etc/profile.d/path-usr-local.sh

# Configure mariadb
systemctl enable mariadb
systemctl start mariadb

mysql_secure_installation << EOF

Y
Y
Testme1234
Testme1234
Y
Y
Y
Y
EOF

# SLURM configuration

mkdir -p /usr/local/etc/slurm/
cat << EOF > /usr/local/etc/slurm/slurm.conf
ClusterName=pwb
SlurmctldHost=localhost
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
TaskPlugin=task/affinity,task/cgroup
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0
SchedulerType=sched/backfill
SelectType=select/cons_tres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageTRES=gres/gpu,gres/shard
GresTypes=gpu,shard
JobCompType=jobcomp/none
JobAcctGatherFrequency=10
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
#NodeName=localhost CPUs=8 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:1,shard:8 State=UNKNOWN
NodeName=localhost CPUs=8 Boards=1 Gres=gpu:v100:1,shard:8 SocketsPerBoard=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=61022 State=UNKNOWN
PartitionName=all Nodes=ALL Default=YES MaxTime=INFINITE State=UP
EOF

cat << EOF > /usr/local/etc/slurm/slurmdbd.conf
AuthType=auth/munge
LogFile=/var/log/slurm/slurmdbd.log
PidFile=/var/run/slurm/slurmdbd.pid
DbdPort=6819
SlurmUser=slurm
StorageType=accounting_storage/mysql

DbdHost=localhost
StorageHost=localhost
StoragePort=3306
StorageLoc=slurm
StorageUser=root
StoragePass=Testme1234
EOF

cat << EOF > /usr/local/etc/slurm/gres.conf
AutoDetect=off
NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia0
NodeName=localhost Name=shard Count=8
#p3.8xlarge
#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia0
#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia1
#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia3
#NodeName=localhost Name=gpu Type=v100 File=/dev/nvidia2
#NodeName=localhost Name=shard Count=8
EOF

cat << EOF > /usr/local/etc/slurm/cgroup.conf
ConstrainCores=yes
ConstrainDevices=yes
ConstrainRAMSpace=yes
EOF

chown slurm /usr/local/etc/slurm/slurmdbd.conf
chmod 0600 /usr/local/etc/slurm/slurmdbd.conf

systemctl enable munge
systemctl enable slurmctld 
systemctl enable slurmdbd
systemctl enable slurmd

```

#### Install and configure Workbench

Workbench will be configured for SLURM Launcher with GRES support only. This will allow the user to specify `shard:X` as GRES where X can run from 1 to 8. CPU and memory resources have been set up so that they are making most efficient use of the available CPUs and memory.

We also will configure support for all 4 IDE's, e.g. RStudio IDE, VS Code, Jupyter Notebooks and JupyterLab. A user named `rstudio` is created that has all admin privileges on Workbench. A random 10-digit password will be assigned and saved in a `.rstudio-pw` file in the `rstudio` users' home-directory.

```{python}
# Workbench installation 

if [ -z $PWB_VERSION ]; then export PWB_VERSION=2024.04.0-daily-675.pro4; fi
yum -y install https://s3.amazonaws.com/rstudio-ide-build/server/rhel9/x86_64/rstudio-workbench-rhel-${PWB_VERSION}-x86_64.rpm

# Workbench setup

configdir="/etc/rstudio"

# Add SLURM integration 
myip=`curl http://checkip.amazonaws.com`

mkdir -p /opt/rstudio/shared-storage

cat > $configdir/launcher-env << EOF
RSTUDIO_DISABLE_PACKAGE_INSTALL_PROMPT=yes
SLURM_CONF=/usr/local/etc/slurm.conf
EOF
 
cat > $configdir/rserver.conf << EOF

# Launcher Config
launcher-address=127.0.0.1
launcher-port=5559
launcher-sessions-enabled=1
launcher-default-cluster=Slurm
launcher-sessions-callback-address=http://${myip}:8787

# Disable R Versions scanning
#r-versions-scan=0

auth-pam-sessions-enabled=1
auth-pam-sessions-use-password=1

# Enable Admin Dashboard
admin-enabled=1
admin-group=rstudio-admins
admin-superuser-group=rstudio-superuser-admins
admin-monitor-log-use-server-time-zone=1
audit-r-console-user-limit-mb=200
audit-r-console-user-limit-months=3

# Enable Auditing
audit-r-console=all
audit-r-sessions=1
audit-r-sessions-limit-mb=512
audit-r-sessions-limit-months=6

EOF

cat > $configdir/launcher.conf<<EOF
[server]
address=127.0.0.1
port=5559
server-user=rstudio-server
admin-group=rstudio-server
authorization-enabled=1
thread-pool-size=4
enable-debug-logging=1

[cluster]
name=Slurm
type=Slurm

#[cluster]
#name=Local
#type=Local

EOF

#cat > $configdir/launcher.slurm.profiles.conf<<EOF 
#[*]
#default-cpus=1
#default-mem-mb=512
##max-cpus=2
#max-mem-mb=1024
#EOF

cat > $configdir/launcher.slurm.resources.conf<<EOF
[small]
name = "Small (1 cpu, 8 GB mem)"
cpus=1
mem-mb=7627
[medium]
name = "Medium (2 cpu, 16 GB mem)"
cpus=2
mem-mb=15255
[large]
name = "Large (4 cpu, 32 GB mem)"
cpus=4
mem-mb=30511
[xlarge]
name = "Large (8 cpu, 64 GB mem)"
cpus=4
mem-mb=61022
EOF

cat > $configdir/launcher.slurm.conf << EOF 
# Enable debugging
enable-debug-logging=1

# Basic configuration
slurm-service-user=slurm
slurm-bin-path=/usr/local/bin

# GPU specifics
enable-gres=1
#gpu-types=v100

EOF

cat > $configdir/jupyter.conf << EOF
jupyter-exe=/usr/local/bin/jupyter
notebooks-enabled=1
labs-enabled=1
EOF

# Install VSCode based on the PWB version.
if ( rstudio-server | grep configure-vs-code ); then rstudio-server configure-vs-code ; rstudio-server install-vs-code-ext; else rstudio-server install-vs-code /opt/rstudio/vscode/; fi
  
cat > $configdir/vscode.conf << EOF
enabled=1
exe=/usr/lib/rstudio-server/bin/code-server/bin/code-server
args=--verbose --host=0.0.0.0 
EOF

# Add sample user 
groupadd --gid 8787 rstudio
useradd -s /bin/bash -m --gid rstudio --uid 8787 rstudio
groupadd --gid 8788 rstudio-admins
groupadd --gid 8789 rstudio-superuser-admins
usermod -G rstudio-admins,rstudio-superuser-admins rstudio

# below SECRET string is a 10-digit random string that will be saved
rspasswd = `tr -dc A-Za-z0-9 < /dev/urandom | head -c 10; echo`
echo $rspasswd > /home/rstudio/.rstudio-pw
echo -e "$rspasswd\n$rspasswd" | passwd rstudio
```

#### Couple of useful tools

```{python}
# Install a couple of useful tools

yum install -y pciutils net-tools nc
```

#### systemctl dependencies

A big part of the deployment is to ensure that the services are being started by `systemd` in the right order. For this, we tweak the systemd service definitions as follows:

-   `slurmdbd` reports it has started but the service is still starting up. This needs to be caught by a `ExecStartPost` script that keeps polling until the `slurmdbd` port is actually open.

-   `slurmd` will only start properly if the nvidia driver has been loaded and the device `/dev/nvidia0` has been created.

-   `nvidia-persistenced` ensures that the nvidia kernel module is loaded - it hence depends on `dkms` which on its own ensures the kernel module for nvidia is being recompiled for any new kernel.

-   `rstudio-server` is only started after `rstudio-launcher` in order to avoid triggering any `Connection refused` messages from `rstudio-server`.

-   `rstudio-server` will start only after `slurmctld` to ensure that SLURM Launcher is starting up properly.

```{python}
# Structure systemctl order of running services

sed -i 's/After=.*/After=dkms.service/' /usr/lib/systemd/system/nvidia-persistenced.service 
sed -i 's/After=/&slurmdbd.service /' /usr/lib/systemd/system/slurmctld.service
sed -i 's/After=/&slurmctld.service /' /usr/lib/systemd/system/slurmd.service
sed -i '/After=.*/a ConditionPathExists=\/dev\/nvidia0' /usr/lib/systemd/system/slurmd.service
sed -i '/ExecReload=.*/a ExecStartPost=\/usr\/bin\/timeout 30 sh -c "while ! ss -H -t -l -n sport = \:6819 | grep -q ^LISTEN.*:6819; do sleep 1; done"' /usr/lib/systemd/system/slurmdbd.service
sed -i 's/After=/&rstudio-launcher.service /' /usr/lib/systemd/system/rstudio-server.service
sed -i 's/After=/&slurmctld.service /' /usr/lib/systemd/system/rstudio-launcher.service
```

#### Reboot

Finally we need to reboot the server in order to get the kernel module built and all the services started properly.

```{python}
reboot
```
